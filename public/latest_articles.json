[
  {
    "title": "Magentic-UI: Towards Human-in-the-loop Agentic Systems",
    "url": "https://arxiv.org/abs/2507.22358",
    "summary": "arXiv:2507.22358v1 Announce Type: new \nAbstract: AI agents powered by large language models are increasingly capable of autonomously completing complex, multi-step tasks using external tools. Yet, they still fall short of human-level performance in most domains including computer use, software development, and research. Their growing autonomy and ability to interact with the outside world, also introduces safety and security risks including potentially misaligned actions and adversarial manipulation. We argue that human-in-the-loop agentic systems offer a promising path forward, combining human oversight and control with AI efficiency to unlock productivity from imperfect systems. We introduce Magentic-UI, an open-source web interface for developing and studying human-agent interaction. Built on a flexible multi-agent architecture, Magentic-UI supports web browsing, code execution, and file manipulation, and can be extended with diverse tools via Model Context Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for enabling effective, low-cost human involvement: co-planning, co-tasking, multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI across four dimensions: autonomous task completion on agentic benchmarks, simulated user testing of its interaction capabilities, qualitative studies with real users, and targeted safety assessments. Our findings highlight Magentic-UI's potential to advance safe and efficient human-agent collaboration.",
    "score": 0.315105,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "ux",
    "category": "collaboration"
  },
  {
    "title": "RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents",
    "url": "https://arxiv.org/abs/2507.22844",
    "summary": "arXiv:2507.22844v1 Announce Type: cross \nAbstract: The development of autonomous agents for complex, long-horizon tasks is a central goal in AI. However, dominant training paradigms face a critical limitation: reinforcement learning (RL) methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths, a problem we term inefficient exploration. This leads to agents that are brittle and fail to generalize, as they learn to find solutions without learning how to reason coherently. To address this, we introduce RLVMR, a novel framework that integrates dense, process-level supervision into end-to-end RL by rewarding verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag its cognitive steps, such as planning, exploration, and reflection, and provides programmatic, rule-based rewards for actions that contribute to effective problem-solving. These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method. On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new state-of-the-art results, with our 7B model reaching an 83.6% success rate on the most difficult unseen task split. Our analysis confirms these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery, leading to more robust, efficient, and interpretable agents.",
    "score": 0.304297,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models",
    "url": "https://arxiv.org/abs/2507.22457",
    "summary": "arXiv:2507.22457v1 Announce Type: new \nAbstract: Recent work has argued that large language models (LLMs) are not \"abstract reasoners\", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an \"abstract reasoner\", and why it matters whether LLMs fit the bill.",
    "score": 0.291735,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs",
    "url": "https://arxiv.org/abs/2507.22716",
    "summary": "arXiv:2507.22716v1 Announce Type: new \nAbstract: Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: https://github.com/probe2/TIRESRAG-R1.",
    "score": 0.287767,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI",
    "url": "https://arxiv.org/abs/2507.22163",
    "summary": "arXiv:2507.22163v1 Announce Type: new \nAbstract: Generative AI opens new possibilities for design exploration by rapidly generating images aligned with user goals. However, our formative study (N=7) revealed three key limitations hindering designers' broad and efficient exploration when interacting with these models. These include difficulty expressing open-ended exploratory intent, lack of continuity in exploration, and limited support for reusing or iterating on previous ideas. We propose IdeaBlocks, where users can express their exploratory intents to generative AI with structured input and modularize them into Exploration Blocks. These blocks can be chained for continuous, non-linear exploration and reused across contexts, enabling broad exploration without losing creative momentum. Our user study with 12 designers showed that participants using IdeaBlocks explored 112.8% more images with 12.5% greater visual diversity than the baseline. They also developed ideas in more iterative and continuous patterns, such as branching, chaining, and revisiting ideas. We discuss design implications for future tools to better balance divergent and convergent support during different phases of exploration, and to capture and leverage exploratory intents more effectively.",
    "score": 0.280438,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "agency",
    "category": "creativity"
  },
  {
    "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering",
    "url": "https://arxiv.org/abs/2412.16936",
    "summary": "arXiv:2412.16936v2 Announce Type: replace \nAbstract: Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively.",
    "score": 0.271672,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "ControlMed: Adding Reasoning Control to Medical Language Model",
    "url": "https://arxiv.org/abs/2507.22545",
    "summary": "arXiv:2507.22545v1 Announce Type: new \nAbstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \\textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \\textit{direct} and \\textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.",
    "score": 0.24947,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making",
    "url": "https://arxiv.org/abs/2507.22365",
    "summary": "arXiv:2507.22365v1 Announce Type: new \nAbstract: In settings where human decision-making relies on AI input, both the predictive accuracy of the AI system and the reliability of its confidence estimates influence decision quality. We highlight the role of AI metacognitive sensitivity -- its ability to assign confidence scores that accurately distinguish correct from incorrect predictions -- and introduce a theoretical framework for assessing the joint impact of AI's predictive accuracy and metacognitive sensitivity in hybrid decision-making settings. Our analysis identifies conditions under which an AI with lower predictive accuracy but higher metacognitive sensitivity can enhance the overall accuracy of human decision making. Finally, a behavioral experiment confirms that greater AI metacognitive sensitivity improves human decision performance. Together, these findings underscore the importance of evaluating AI assistance not only by accuracy but also by metacognitive sensitivity, and of optimizing both to achieve superior decision outcomes.",
    "score": 0.247223,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "ux",
    "category": "collaboration"
  },
  {
    "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection",
    "url": "https://arxiv.org/abs/2505.19010",
    "summary": "arXiv:2505.19010v2 Announce Type: replace \nAbstract: Multi-modal learning has emerged as a crucial research direction, as integrating textual and visual information can substantially enhance performance in tasks such as classification, retrieval, and scene understanding. Despite advances with large pre-trained models, existing approaches often suffer from insufficient cross-modal interactions and rigid fusion strategies, failing to fully harness the complementary strengths of different modalities. To address these limitations, we propose Co-AttenDWG, co-attention with dimension-wise gating, and expert fusion. Our approach first projects textual and visual features into a shared embedding space, where a dedicated co-attention mechanism enables simultaneous, fine-grained interactions between modalities. This is further strengthened by a dimension-wise gating network, which adaptively modulates feature contributions at the channel level to emphasize salient information. In parallel, dual-path encoders independently refine modality-specific representations, while an additional cross-attention layer aligns the modalities further. The resulting features are aggregated via an expert fusion module that integrates learned gating and self-attention, yielding a robust unified representation. Experimental results on the MIMIC and SemEval Memotion 1.0 datasets show that Co-AttenDWG achieves state-of-the-art performance and superior cross-modal alignment, highlighting its effectiveness for diverse multi-modal applications.",
    "score": 0.229395,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "ux",
    "category": "modalities"
  },
  {
    "title": "An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem",
    "url": "https://arxiv.org/abs/2507.22326",
    "summary": "arXiv:2507.22326v1 Announce Type: new \nAbstract: Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.",
    "score": 0.182745,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "agency",
    "category": "companion"
  },
  {
    "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach",
    "url": "https://arxiv.org/abs/2507.22671",
    "summary": "arXiv:2507.22671v1 Announce Type: new \nAbstract: Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools.",
    "score": 0.155677,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "ux",
    "category": "research-assistant"
  },
  {
    "title": "Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer",
    "url": "https://arxiv.org/abs/2407.10247",
    "summary": "arXiv:2407.10247v2 Announce Type: replace-cross \nAbstract: The integration of Artificial Intelligence (AI) into corporate strategy has become critical for organizations seeking to maintain a competitive advantage in the digital age. As AI transforms business models, operations, and decision-making, the need for dedicated executive leadership to guide, govern, and orchestrate this transformation becomes increasingly evident. This paper examines emerging future scenarios across three domains: the AI Economy, the AI Organization, and Competition in the Age of AI. These domains reveal environmental, structural, and strategic tensions that existing C-suite roles struggle to resolve. In response, the paper develops a theory-informed framework for the Chief AI Officer (CAIO), outlining the distinct functions and capabilities required to guide and govern AI at scale. Drawing on illustrative cases and emerging practice, this conceptualization clarifies the CAIOs unique role within the executive landscape and presents a forward-looking research agenda. This paper advances the discourse on AI leadership by offering a theory-driven rationale for the strategic integration of AI at the executive level and by positioning the Chief AI Officer as a distinct and necessary role within modern organizations.",
    "score": 0.148698,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "society",
    "category": "work-transformation"
  },
  {
    "title": "Exploring Student-AI Interactions in Vibe Coding",
    "url": "https://arxiv.org/abs/2507.22614",
    "summary": "arXiv:2507.22614v1 Announce Type: new \nAbstract: Background and Context. Chat-based and inline-coding-based GenAI has already had substantial impact on the CS Education community. The recent introduction of ``vibe coding'' may further transform how students program, as it introduces a new way for students to create software projects with minimal oversight.\n  Objectives. The purpose of this study is to understand how students in introductory programming and advanced software engineering classes interact with a vibe coding platform (Replit) when creating software and how the interactions differ by programming background.\n  Methods. Interview participants were asked to think-aloud while building a web application using Replit. Thematic analysis was then used to analyze the video recordings with an emphasis on the interactions between the student and Replit.\n  Findings. For both groups, the majority of student interactions with Replit were to test or debug the prototype and only rarely did students visit code. Prompts by advanced software engineering students were much more likely to include relevant app feature and codebase contexts than those by introductory programming students.",
    "score": 0.144431,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "society",
    "category": "education"
  },
  {
    "title": "Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America",
    "url": "https://arxiv.org/abs/2505.08841",
    "summary": "arXiv:2505.08841v2 Announce Type: replace-cross \nAbstract: As artificial intelligence and robotics increasingly reshape the global labor market, understanding public perceptions of these technologies becomes critical. We examine how these perceptions have evolved across Latin America, using survey data from the 2017, 2018, 2020, and 2023 waves of the Latinobar\\'ometro. Drawing on responses from over 48,000 individuals across 16 countries, we analyze fear of job loss due to artificial intelligence and robotics. Using statistical modeling and latent class analysis, we identify key structural and ideological predictors of concern, with education level and political orientation emerging as the most consistent drivers. Our findings reveal substantial temporal and cross-country variation, with a notable peak in fear during 2018 and distinct attitudinal profiles emerging from latent segmentation. These results offer new insights into the social and structural dimensions of AI anxiety in emerging economies and contribute to a broader understanding of public attitudes toward automation beyond the Global North.",
    "score": 0.119835,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "society",
    "category": "future"
  },
  {
    "title": "Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic Matching Approach",
    "url": "https://arxiv.org/abs/2507.22504",
    "summary": "arXiv:2507.22504v1 Announce Type: new \nAbstract: The post-pandemic surge in healthcare demand, coupled with critical nursing shortages, has placed unprecedented pressure on emergency department triage systems, necessitating innovative AI-driven solutions. We present a multi-agent interactive intelligent system for medical triage that addresses three fundamental challenges in current AI-based triage systems: insufficient medical specialization leading to hallucination-induced misclassifications, heterogeneous department structures across healthcare institutions, and inefficient detail-oriented questioning that impedes rapid triage decisions. Our system employs three specialized agents - RecipientAgent, InquirerAgent, and DepartmentAgent - that collaborate through structured inquiry mechanisms and department-specific guidance rules to transform unstructured patient symptoms into accurate department recommendations. To ensure robust evaluation, we constructed a comprehensive Chinese medical triage dataset from a medical website, comprising 3,360 real-world cases spanning 9 primary departments and 62 secondary departments. Through systematic data imputation using large language models, we address the prevalent issue of incomplete medical records in real-world data. Experimental results demonstrate that our multi-agent system achieves 89.2% accuracy in primary department classification and 73.9% accuracy in secondary department classification after four rounds of patient interaction. The system's pattern-matching-based guidance mechanisms enable efficient adaptation to diverse hospital configurations while maintaining high triage accuracy. Our work provides a scalable framework for deploying AI-assisted triage systems that can accommodate the organizational heterogeneity of healthcare institutions while ensuring clinically sound decision-making.",
    "score": 0.109686,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "society",
    "category": "healthcare"
  },
  {
    "title": "AI Energy Paradox: Powering the Future Without Burning It Down",
    "url": "https://ai.gopubby.com/ai-energy-paradox-powering-the-future-without-burning-it-down-752396678a6e?source=rss----3fe99b2acc4---4",
    "summary": "<p>If you\u2019ve been reading the headlines about AI and energy, you\u2019d think we\u2019re on the brink of a GPU-powered environmental meltdown. The story practically writes itself: colossal data centers glowing in the dark, guzzling enough power to keep small nations lit, all so we can generate memes or ask ChatGPT to draft our\u00a0emails.</p><p><em>It\u2019s a seductive narrative. It\u2019s also a partial\u00a0one.</em></p><p>Yes, AI\u2019s energy demand is rising quickly. That\u2019s real. And it\u2019s big enough to be worth serious attention. But it\u2019s also just one part of a much bigger picture\u200a\u2014\u200aa picture that includes dramatic leaps in hardware efficiency, ambitious clean energy buildouts, smarter software design, and a real possibility that AI itself can help us <em>save</em> energy in other\u00a0sectors.</p><p>In the middle of this debate, the real question isn\u2019t whether AI is \u201cgood\u201d or \u201cbad\u201d for our energy future. It\u2019s whether we have the will and foresight to design that future intentionally.</p><p>Because energy transitions are never automatic. They\u2019re designed, planned, and fought\u00a0for.</p><p>In the sections below, I\u2019ll look at our energy journey to-date, today\u2019s challenges, and a look ahead to how we\u2019re addressing them.</p><h3><strong>I: Current Consumption: The Real Numbers and Misconceptions</strong></h3><p>First, let\u2019s talk about the real scale of the problem\u200a\u2014\u200aand clear up some sloppy thinking.</p><p><strong>Growing Demand</strong></p><p>When people talk about \u201cAI\u2019s power demands,\u201d they often forget the basics. A <strong>megawatt (MW)</strong> is a rate: how much power a system needs right now. A <strong>megawatt-hour (MWh)</strong> is total energy used over time. It\u2019s like saying my house used 188 kilowatt-hours (kWh) last Sunday\u200a\u2014\u200aa number that means something only when you know it\u2019s spread over a day. And I know\u2026 a high number for a single\u00a0home.</p><p><em>Now scale that\u00a0up.</em></p><p>A single hyperscale data center might need tens of MWs continuously. But the new AI superclusters being planned? They\u2019re aiming for <em>gigawatt</em> scale. That\u2019s 1,000 MW\u200a\u2014\u200aenough to power all of San Francisco.</p><p>According to the International Energy Agency (IEA), <strong>global data center electricity consumption hit around 400 TWh in 2024</strong>. That\u2019s roughly equivalent to running 400 large nuclear reactors full tilt, with the US responsible for almost half of this. And new AI workloads are projected to increase that number dramatically.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*wt9_aM6c7Jtd9xP4q1-0TQ.png\" /></figure><p>By 2030, the industry projects data center demand could hit <strong>over 1,100 TWh</strong>, more than tripling in just a few years. By 2040, some forecasts suggest it could exceed <strong>2,200\u00a0TWh</strong>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*sNxbhDZ0N9XZEnow2rRguw.png\" /><figcaption>Source: [1]</figcaption></figure><p>Expanding the picture, what about overall consumption? In the US, after remaining relatively flat for two decades, the curve is growing [2] once again, due to electrification of households, EVs, and overall manufacturing growth. And much of the new capacity constructed to meet this need, at least in the near-term, will rely on fossil fuels vs leveraging renewables or focusing on efficiency.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*JWnBVIIS9yvOc5Ig6SwHmg.png\" /><figcaption>Source: [2]</figcaption></figure><p>The <strong>4,000 TWh</strong> in the above chart aligns with the 200 TWh for US data centers, or close to 5% of overall consumption described earlier.</p><p><em>These are big numbers. And they\u00a0matter.</em></p><p><em>But they also need\u00a0context.</em></p><p><em>Because not all AI power consumption is the\u00a0same.</em></p><p><strong>Training vs. Inference: The Hidden\u00a0Shift</strong></p><p>You\u2019ll often hear about how training a massive model like GPT-3 used ~1,300 MWh\u200a\u2014\u200aenough to power 130 average U.S. homes for a year. That\u2019s a big, scary number. And it\u2019s real. But training is a <em>one-time</em>\u00a0cost.</p><p>The flip side of the power consumption coin is the carbon footprint, and the recent Stanford AI Index Report [3] includes a great slide that puts this in perspective. Remember the 1,100 TWh number above? Total emissions are projected to be 4.5% of the global total, on-par with the entire utility/waste sector\u00a0[4].</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/890/1*JUXJgiJMtwEdgUKfBCdeDg.png\" /><figcaption>Source: [3]</figcaption></figure><p>Inference is the bigger long-term issue. That\u2019s the energy spent every single time you prompt a model to write code, explain history, or generate an\u00a0image.</p><p>Today, inference already dwarfs training in aggregate power use. In 2024, ChatGPT alone was estimated [5] to consume over <strong>1 billion kWh in the U.S.</strong> That\u2019s roughly enough to power over 100,000 American homes for a year. And that figure will only grow as generative AI becomes more integrated into our daily lives and business processes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*2UKEMfvcZNLHD2r38OIyjQ.png\" /><figcaption>Source: [6]</figcaption></figure><p>This is why the conversation about energy must move beyond the one-time cost of training runs. The real sustainability question is: <strong>How efficient can inference become at scale? </strong>Are models like DeepSeek-R1 more efficient, and what can we push further to the edge? I\u2019ll cover this in a\u00a0bit.</p><p><strong>The Per-Query Myth</strong></p><p>And let\u2019s clear up another favorite headline: <em>\u201cAn AI query uses 10\u201315x more energy than a Google\u00a0search!\u201d</em></p><p>True? Sort of. But also, deeply misleading.</p><p>A classic Google search gives you ten blue links. You click around. Load multiple pages. Repeat searches. AI chat compresses multiple steps\u200a\u2014\u200aquery refinement, summarization, extraction\u200a\u2014\u200ainto one. And it also probably uncovers insights that traditional search can\u2019t\u00a0address.</p><p>Moreover, the carbon intensity of the power source matters. Michael Barnard in \u2018The Future is Electric\u2019 offers [7] his perspective, putting the average cost per query into\u00a0context:</p><p>\u00b7 Microsoft\u2019s Azure datacenter in Quincy averages about <strong>0.019 kg CO2e per kWh</strong>. That means a typical ChatGPT query might have a carbon cost of <strong>0.03 grams\u00a0CO2e</strong>.</p><p>\u00b7 For comparison? Your morning cup of coffee is ~21\u00a0grams.</p><p><strong>Context Matters: AI vs. Human Emissions</strong></p><p>And there\u2019s an even more overlooked angle.</p><p>We often forget that AI is replacing <em>human</em> labor\u200a\u2014\u200alabor that itself has an energy and carbon\u00a0cost.</p><p>Analyst Michael Barnard puts this in perspective:</p><p>\u201cYou\u2019d have to generate 4 billion AI images to match the annual emissions of the average American\u00a0driver.\u201d</p><p>Similarly, Will Lockett\u2019s writing at \u2018Predict\u2019 notes [8] that generative AI systems produce <strong>130\u20131500 times less CO2 per page of text</strong> than human authors, and <strong>310\u20132900 times less CO2 per image</strong> than human\u00a0artists.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*CASX4Zbm0zuFdKTd4h6Fbw.png\" /><figcaption>Source: [9]</figcaption></figure><p>So, while AI uses energy, it also has the potential to <strong>displace</strong> far more carbon-intensive human production.</p><p>This is not to minimize its energy footprint. But it reframes the question:</p><p><em>Not how much energy does AI use, but how much does it save compared to the alternative?</em></p><p><strong>The Broader Data Center\u00a0Context</strong></p><p>It\u2019s also crucial to see AI in the broader world of data\u00a0centers.</p><p>Traditional cloud workloads, video streaming, social media, storage\u200a\u2014\u200athey all contribute. The AI workload is growing as a share, but it doesn\u2019t exist in a\u00a0vacuum.</p><p>In fact, some analysts argue that AI might <em>improve</em> overall data center efficiency by making routing, cooling, and load-balancing smarter.</p><p>Data centers are already moving from generic compute to specialized AI clusters with higher utilization rates. That could reduce waste, flatten peak loads, and even enable better demand-response integration with\u00a0grids.</p><p>It\u2019s complicated. But that\u2019s the\u00a0point.</p><p>The problem isn\u2019t \u201cAI uses energy.\u201d The problem is <em>how</em> we generate that energy, <em>how</em> we allocate it, and <em>how</em> we design for efficiency at every\u00a0layer.</p><h3><strong>II: Current Progress: Building for Efficiency</strong></h3><p><em>Despite the headlines, AI companies aren\u2019t blind to these challenges.</em></p><p><strong>Smarter Chips</strong></p><p>First off, smarter chip architectures. Nvidia is probably at the forefront, with their Blackwell GPUs promising <strong>25x better energy efficiency</strong> for training and inference compared to earlier designs. The chart below from the Stanford AI Index Report [10] best depicts progress over the past eight\u00a0years.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*lSFNbyR_TMuO6LU-9usGMQ.png\" /><figcaption>Source: [10]</figcaption></figure><p>And other are not asleep at the switch. Google\u2019s TPU progression improved energy use by <strong>67%</strong> in just two years, and OpenAI says its model training efficiency has improved <strong>350x since\u00a02016</strong>.</p><p>This isn\u2019t marketing fluff\u200a\u2014\u200ait\u2019s essential data. Energy-per-computation has been halving roughly every 18 months, aligned with Koomey\u2019s Law [11], a corollary of sorts to Moore\u2019s\u00a0Law.</p><p><em>That trend is vital to keep AI\u2019s energy growth in check, even as demand skyrockets.</em></p><p><strong>Small Language Models and Edge\u00a0AI</strong></p><p>Perhaps the biggest near-term win? <strong>Moving inference to the\u00a0edge.</strong></p><p>Instead of sending every prompt to giant GPU clusters in the cloud, small language models (SLMs) can run locally\u200a\u2014\u200aon your phone or laptop. This is part of \u2018Green AI\u2019 that everyone is talking about. A very relevant observation: [12]</p><p>\u00b7 <em>For many tasks, like summarizing documents or generating images, large models can be overkill\u200a\u2014\u200athe equivalent of driving a tank to pick up groceries.</em></p><p>\u00b7 <em>\u201cIt shouldn\u2019t take quadrillions of operations to compute 2 + 2,\u201d said Illia Polosukhin, who currently works on blockchain technology and was one of the authors of a seminal 2017 Google paper that laid the foundation for the current generative AI\u00a0boom.</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*hOb3z9zYy_kFGPS67zEtPw.png\" /><figcaption>Sources: [13] [14]\u00a0[15]</figcaption></figure><p>As an example, Apple Intelligence system routes many queries to on-device SLMs, only using cloud GPUs, either Apple\u2019s or OpenAI\u2019s, for complex requests.</p><p>Other examples include Experian replacing massive LLMs with smaller, more efficient models for customer service. Same outcome, dramatically less power, as related by Ali Khan, the company\u2019s chief data officer. And Meta\u2019s MobileLLM program pushing generative AI into smartphones, even with DRAM and battery constraints. For perspective, the graphic below [16] depicts this challenge, with the higher-speed SRAM Cache and DRAM too small to support most existing SLMs. But this is where all the research and innovations are taking place, such as Apple\u2019s \u2018LLM in a flash: Efficient Large Language Model Inference with Limited Memory.\u00a0[17]\u2019</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*1rVUijKScscB0P2K5D9fEQ.png\" /><figcaption>Source: [16]</figcaption></figure><p>This change matters. A request served locally avoids the energy (and latency) cost of roundtripping to a hyperscale datacenter.</p><p><em>This isn\u2019t marginal. It\u2019s a structural shift in inference energy cost\u00a0curves!</em></p><p><strong>Data Center Design Innovation</strong></p><p>It\u2019s also about <em>how</em> data centers are built and operated.</p><p>Some use immersion cooling\u200a\u2014\u200asubmerging servers in dielectric liquid\u200a\u2014\u200ato eliminate inefficient HVAC systems. And others are experimenting with greater use of optical networking to reduce power lost in electrical switching.</p><p>Waste heat reuse is emerging too. A new San Jose project plans to pipe data center heat to local housing developments, offsetting natural gas demand and reducing the carbon footprint. From the \u2018Mercury News\u2019 reporting [18]:</p><p>\u00b7 <em>\u201cYou could have an entire downtown powered by data centers,\u201d Ian Gillespie, Westbank\u2019s chief executive officer and founder, said Wednesday.</em></p><p>\u00b7 <em>\u201cOur long-term vision is with multiple data centers and housing clusters, the idea is to connect them all together and create a downtown San Jose district energy system,\u201d Jacobson said. \u201cIf we can capture low-cost, low-carbon energy, that creates a huge opportunity downtown</em></p><p>Initiatives like this are top-of-mind for the Bay Area and elsewhere. On a positive note, the city has recently struck a deal with PG&amp;E to provide 2 GW of new power capacity over the coming years, primarily for data centers [18a]. Little known fact: 60% of the power generated in Santa Clara is dedicated to data centers\u00a0[19].</p><p><em>All these moves don\u2019t eliminate the energy draw. But they pave the way for a far more efficient future.</em></p><p><strong>Clean Energy Procurement</strong></p><p>Then there\u2019s the question of where the power comes from, the real elephant in the room. And it is a complex issue, spanning generation, transmission, storage, and near-term fossil fuel fallback.</p><p>First off, the cloud giants aren\u2019t ignoring\u00a0this.</p><ul><li>Microsoft is buying [20] nuclear power from the Three Mile Island facility, investing in fusion research, and supporting carbon\u00a0capture.</li><li>Amazon and Google are exploring <strong>small modular reactors (SMRs)</strong>\u200a\u2014\u200acompact, factory-built nuclear\u00a0designs.</li><li>Meta signed [21] a deal for 150 MW of geothermal power, using advanced drilling similar to fracking. Not a large deal, but in any case, innovative.</li></ul><p>Closer to home, California went from <strong>55 MW of solar to over 20 GW</strong> in a\u00a0decade.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*2Jq876K7pyISK54p-3wugA.png\" /><figcaption>Sources: [22]\u00a0[23]</figcaption></figure><p>And to help with the generation and consumption curve, PG&amp;E is extending the lifetime of the Diablo Nuclear Station, initially a very contentious plan due to environmental concerns.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*XHpT3vKuYV4yynBRaZpoeQ.png\" /><figcaption>Source: [24]</figcaption></figure><p>And cloud providers like AWS and Microsoft have been locating their data centers close to hydropower for over a decade [25], particularly in the Pacific Northwest and within the TVA footprint. But it is not just in the US where changes are\u00a0afoot.</p><p>Moving to the Middle East, also a hotbed of solar deployment, Datavolt is building hybrid datacenter campuses with solar, wind, and limited fossil fallback, aiming for <strong>5 GW</strong> of capacity. And plans are in the works for British Columbia from Bell Canada [26] and\u00a0others.</p><p>These aren\u2019t side projects. They\u2019re integral to supporting the next wave of AI. On a global scale, renewables <em>are</em> making a difference.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*SAhBH5QSWE4bLPAUxFGlSA.png\" /><figcaption>Source: [27]</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*Kvim1WjgEjQg8y6WAnuQUQ.png\" /><figcaption>Sources: [28] [29] [30]\u00a0[31]</figcaption></figure><p>But there are challenges. Inference doesn\u2019t turn off when the sun goes down! Looking back to California, solar peaks during the day. Today, we only have so much battery backup capacity, though it has surged [32] to over <strong>15 GW</strong> over the past five years. And batteries come with their own grid interconnect challenges and safety\u00a0issues.</p><p>On the positive side, over the past decade, the cost of battery backup has gone down substantially, more-or-less following the cost of solar panels as well. Not to mention the positive impact on EVs, this makes large scale backup facilities financially viable. In fact, reported [33] by \u2018The Information\u2019 and quoting analysis by BloombergNEF, the price for stationary storage is down to $21/kWh, at least in China, due to excess capacity. Costs could be higher in the US due to import restrictions.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*UdxVVXfEgYTtAdRW8nxEkg.png\" /><figcaption>Source: [34]</figcaption></figure><p>And if battery storage has become mainstream, how do we connect renewables to this backup, and then to where the power is needed? This is the real challenge, and unfortunately, we\u2019re not making progress as quicky as hoped due to a set of interrelated issues.</p><p><em>Basically, the U.S. grid simply isn\u2019t\u00a0ready.</em></p><p>Permitting delays are a killer. A data center can be built in <strong>2 years</strong>. But connecting new solar or wind projects can take <strong>5\u201310 years</strong> [35]<strong> </strong>due to interconnection queues and local opposition.</p><p>This mismatch is why builders often resort to natural gas peaker plants\u200a\u2014\u200afast, flexible, but carbon-intensive. And countries or even regions in the US with limited grid diversity face stark trade-offs.</p><p>For example, Ireland\u2019s data centers now use over <strong>20%</strong> of the country\u2019s electricity. Much of that is still fossil-based, undermining climate goals. Due to this, the country is slowing [36] down the rate of new data center construction. Similar dynamics are emerging in U.S. states with high datacenter growth but slow grid modernization.</p><p><em>Where then are we, how did we get here, and where do we need to\u00a0go?</em></p><p><strong>The Grid</strong></p><p>The US grid is aging, as evidenced by increasing outages over the past few years, and these are not due to data center growth. As mentioned above, permitting is a killer, and the US grid itself is disjointed due to historical reasons. But looking at the map below, the issue is that renewables are not located close to great grid connectivity. Solving this with require a major undertaking on the scale of the New\u00a0Deal.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*52yFtCfr_Au49ckExVy20A.png\" /><figcaption>Source: [37]</figcaption></figure><p>And the map below from 2025 is an updated view, clearly showing the lack of high-capacity grid connectivity between where some of the renewables, both solar and wind, are most abundant, and the coasts that depend upon their\u00a0output.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*b6ozFufpyzQYjHoFbEOzHQ.png\" /><figcaption>Source: [38]</figcaption></figure><p>As a recent \u2018CNN\u2019 opinion piece points [39]\u00a0out:</p><p>\u00b7 <em>Within the decade, we\u2019re looking at a grid infrastructure that can\u2019t keep pace with rapidly rising demand, even with all sources running around the clock. This pushes us toward a scenario unimaginable for most Americans\u200a\u2014\u200aan unreliable energy grid, marked by blackouts and rolling brownouts. Recall the blackouts</em> [40]<em> across Texas in winter 2021 and the 2003 East Coast blackout </em>[41]<em> that impacted 50 million people. But this time, it won\u2019t be a stroke of bad luck linked to extreme weather or a safety incident. Electricity rationing will become a standard part of American life while the grid races to add capacity.</em></p><p>\u00b7 <em>A truly national grid demands a singular federal oversight body. Such a consolidation of authority is proposed in the Streamlining Interstate Transmission of Electricity (SITE) </em>[42] <em>Act. This, in concert with a nationwide push to build and finance new electricity generation and transmission\u200a\u2014\u200aon the scale of New Deal era construction\u200a\u2014\u200ais needed to meet the energy demand of an AI-powered, green\u00a0economy.</em></p><p><em>If the West wants to lead in AI sustainably, it will need to match that ambition in energy infrastructure.</em></p><p><strong>A Last Word on Investments and Challenges</strong></p><p>Separate from investments specifically in renewables, the vendor community at-large has woken up to energy investment. CB Insights does a good job of capturing [43] the various sub-industries within the larger energy ecosystem, along with key investors. What is interesting is that you wouldn\u2019t expect Amazon or Nvidia to be so\u00a0active.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*KaFMiWoclAPPKSGNNJy7gw.png\" /><figcaption>Source: [44]</figcaption></figure><p>And hopefully innovations from the above will help mitigate issues uncovered \u2018McKinsey\u2019 research [45] that highlights realities standing in front of a true energy revolution.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*6EaEOgFobSzfjV3m2HKBEA.png\" /><figcaption>Source: [45]</figcaption></figure><p><strong>AI as an Energy Optimizer</strong></p><p>And let\u2019s not forget AI\u2019s potential to <em>save</em> energy in other\u00a0sectors.</p><ul><li>NVIDIA worked with Foxconn to optimize a factory, cutting energy demand by\u00a0<strong>30%</strong>.</li><li>Google\u2019s AI-powered contrail avoidance can reduce aviation-induced warming.</li><li>Utilities use AI for predictive maintenance and demand balancing, reducing overgeneration and\u00a0waste.</li></ul><p>Taking a broader view, McKinsey in their recent publication, \u2018Beyond the hype, Capturing the potential of AI and gen AI in TMT,\u2019 estimates [46] generative AI could add <strong>$2.6 to $4.4 trillion</strong> annually in value, much of it by improving resource efficiency and productivity. The figure below breaks this down by function.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*GXOmaGbfqQHXQ7S8IUGIJg.png\" /><figcaption>Source: [46]</figcaption></figure><p>Some of the productivity gains outlined above are best articulated in [47] a recent interview with Bill\u00a0Gates:</p><p>\u00b7 <em>Data centers will drive a rise in global electricity usage of between 2\u20136%, Gates said. But added: \u201cThe question is will AI accelerate a more than 6% reduction? And the answer is: certainly.\u201d</em></p><h3><strong>III. Looking Forward: Future Technologies and Bold\u00a0Ideas</strong></h3><p>This isn\u2019t just a question of scaling current\u00a0tech.</p><p>It\u2019s about investing in <em>new</em> paradigms.</p><p><strong>Advanced Hardware\u00a0Designs</strong></p><p>IBM and others are developing analog computing [48] using phase-change materials to emulate synapses, slashing energy use per operation, and other are looking into thermodynamic computing [49]\u00a0[50],</p><p>Photonic computing [51] replaces electrical signals with light, reducing resistance and\u00a0heat.</p><p>Neuromorphic [52] chips model computation on the brain\u2019s efficiency, potentially cutting energy needs by orders of magnitude for certain\u00a0tasks.</p><p>Father afield, there is research into high temperature superconductors [53] to reduce transmission loss and build grid resiliency, as well as the integration of quantum computing with AI, AI Quantum (AQ)\u00a0[54].</p><p>These aren\u2019t theoretical\u200a\u2014\u200athey\u2019re being prototyped now.</p><p><strong>Energy Source Innovation</strong></p><p>Fusion isn\u2019t a joke anymore. Commonwealth Fusion Systems aims for its first pilot plant in the next few years, and as mentioned earlier, <strong>Advanced geothermal</strong> is unlocking new regions with fracking-like drilling to reach heat reservoirs.</p><p><strong>Systemic Policy and Business\u00a0Models</strong></p><p>Even the way we think about AI deployment is changing.</p><p>OpenAI\u2019s proposal for <strong>AI Economic Zones </strong>[55]<strong> </strong>envisions fast-tracked permitting, dedicated clean power development, and shared data infrastructure as public-private partnerships.</p><p>It\u2019s an acknowledgment that this isn\u2019t just a technical problem\u200a\u2014\u200ait\u2019s a regulatory, economic, and political one.</p><p>Conrad Gray penning in \u2018Humanity Redefined\u2019 also offers [56] his perspective.</p><p><em>To meet the growing demand for AI compute and energy, OpenAI proposes initiatives such as AI Economic Zones to fast-track infrastructure projects, increased investment in new energy sources, and a National AI Infrastructure Highway to connect regional power and communication grids. It also advocates digitising public data to support US AI developers of all sizes. In return, OpenAI suggests, developers using this data could collaborate with the government to generate insights that inform the development of better public policies.</em></p><h3><strong>Conclusion: The Call to\u00a0Action</strong></h3><p>We\u2019re not doomed. But we are challenged.</p><p>AI\u2019s energy hunger is real. But so is its potential to help us save energy elsewhere, to drive investment in clean generation, to make the grid smarter, to accelerate hardware breakthroughs.</p><p>The question is whether we\u2019ll treat energy efficiency as an afterthought\u200a\u2014\u200aor as a core design\u00a0goal.</p><p>We\u2019ve faced transformations like this\u00a0before.</p><p>We met the demands of industrialization, electrification, the internet revolution\u200a\u2014\u200aby reinventing our energy systems and our technologies.</p><p>We can do it\u00a0again.</p><p><strong>But only if we choose\u00a0to.</strong></p><h3>References:</h3><p>[1]. Source: Datavolt analysis of data from IEA, IRENA, and\u00a0WSJ</p><p>[2]. Plumer, B. (2025). How electric power affects climate change. <a href=\"https://www.nytimes.com/interactive/2024/03/13/climate/electric-power-climate-change.html\">https://www.nytimes.com/interactive/2024/03/13/climate/electric-power-climate-change.html</a></p><p>[3]. Stanford Institute for Human-Centered Artificial Intelligence. (2025). <em>AI Index Report 2025</em>. <a href=\"https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf\">https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf</a></p><p>[4]. Datavolt analysis of data from IEA, IRENA, and\u00a0WSJ</p><p>[5]. BestBrokers. (2025). AI\u2019s power demand: Calculating ChatGPT\u2019s electricity consumption for over 78 billion user queries annually. <a href=\"https://www.bestbrokers.com/forex-brokers/ais-power-demand-calculating-chatgpts-electricity-consumption-for-handling-over-78-billion-user-queries-every-year/\">https://www.bestbrokers.com/forex-brokers/ais-power-demand-calculating-chatgpts-electricity-consumption-for-handling-over-78-billion-user-queries-every-year/</a></p><p>[6]. BestBrokers. (2025). AI\u2019s power demand: Calculating ChatGPT\u2019s electricity consumption for over 78 billion user queries annually. <a href=\"https://www.bestbrokers.com/forex-brokers/ais-power-demand-calculating-chatgpts-electricity-consumption-for-handling-over-78-billion-user-queries-every-year/\">https://www.bestbrokers.com/forex-brokers/ais-power-demand-calculating-chatgpts-electricity-consumption-for-handling-over-78-billion-user-queries-every-year/</a></p><p>[7]. Barnard, M. (2024). AI Isn\u2019t Eating All Electricity &amp; Emitting Massive Carbon. <a href=\"https://medium.com/the-future-is-electric/ai-isnt-eating-all-electricity-emitting-massive-carbon-18718ce2315d\">https://medium.com/the-future-is-electric/ai-isnt-eating-all-electricity-emitting-massive-carbon-18718ce2315d</a></p><p>[8]. Lockett, W. (2024). AI Has A Giant, Destructive Flaw. <a href=\"https://medium.com/predict/ai-has-a-giant-destructive-flaw-e20e49f8813d\">https://medium.com/predict/ai-has-a-giant-destructive-flaw-e20e49f8813d</a></p><p>[9]. Lockett, W. (2024). AI Has A Giant, Destructive Flaw. <a href=\"https://medium.com/predict/ai-has-a-giant-destructive-flaw-e20e49f8813d\">https://medium.com/predict/ai-has-a-giant-destructive-flaw-e20e49f8813d</a></p><p>[10]. Stanford Institute for Human-Centered Artificial Intelligence. (2025). <em>AI Index Report 2025</em>. <a href=\"https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf\">https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf</a></p><p>[11]. Wikipedia contributors. (2025). <em>Koomey\u2019s Law</em>. <a href=\"https://en.wikipedia.org/wiki/Koomey%27s_law\">https://en.wikipedia.org/wiki/Koomey%27s_law</a></p><p>[12]. Dolan, T., and Seetharaman, D. (2024). For AI Giants, Smaller Is Sometimes Better. <a href=\"https://www.wsj.com/tech/ai/for-ai-giants-smaller-is-sometimes-better-ef07eb98?st=yjdcudht34zidbk&amp;reflink=article_imessage_share\">https://www.wsj.com/tech/ai/for-ai-giants-smaller-is-sometimes-better-ef07eb98?st=yjdcudht34zidbk&amp;reflink=article_imessage_share</a></p><p>[13]. Google, Meta, Qualcomm, &amp; MLPerf. (2025). Estimated energy consumption per AI query: Cloud vs. edge inference. Aggregated data analysis showing cloud inference at 500\u20131,000 mJ and edge inference at 20\u201350 mJ per query, resulting in a 20x efficiency ratio.</p><p>[14]. Lokwon, K. (2025). How on-device AI can help us cut AI\u2019s energy demand. <a href=\"https://www.weforum.org/stories/2025/03/on-device-ai-energy-system-chatgpt-grok-deepx/\">https://www.weforum.org/stories/2025/03/on-device-ai-energy-system-chatgpt-grok-deepx/</a></p><p>[15]. Chai, S. (2025). How Edge Computing Can Solve AI\u2019s Energy Crisis. <a href=\"https://builtin.com/artificial-intelligence/edge-ai-energy-solution\">https://builtin.com/artificial-intelligence/edge-ai-energy-solution</a></p><p>[16]. De Gregorio. (2024). Meta Takes On AI\u2019s Elephant in the Room. <a href=\"https://medium.com/@ignacio.de.gregorio.noblejas/meta-takes-on-ais-elephant-in-the-room-4e2511265afd\">https://medium.com/@ignacio.de.gregorio.noblejas/meta-takes-on-ais-elephant-in-the-room-4e2511265afd</a></p><p>[17]. Alizadeh, K. et al. (2024). LLM in a flash: Efficient Large Language Model Inference with Limited Memory. <a href=\"https://arxiv.org/pdf/2312.11514\">LLM in a flash: Efficient Large Language Model Inference with Limited\u00a0Memory</a></p><p>[18]. Avalos, G. (2025). Housing and data center combo may sprout this year in downtown San Jose. <a href=\"https://www.mercurynews.com/2025/03/19/san-jose-home-build-property-economy-pge-energy-electric-develop-house/\">https://www.mercurynews.com/2025/03/19/san-jose-home-build-property-economy-pge-energy-electric-develop-house/</a></p><p>[18a]. Patel, D. (2025). San Jose strikes deal with PG&amp;E, committing utility to delivering major grid improvements. <a href=\"https://www.mercurynews.com/2025/07/25/san-jose-strikes-deal-pge-data-center-energy-investment/\">https://www.mercurynews.com/2025/07/25/san-jose-strikes-deal-pge-data-center-energy-investment/</a></p><p>[19]. Moss, S. (2023). Silicon Valley Power says data center load to double by 2035, will need geothermal power and batteries. <a href=\"https://www.datacenterdynamics.com/en/news/silicon-valley-power-says-data-center-load-to-double-by-2035-will-need-geothermal-power-and-batteries\">https://www.datacenterdynamics.com/en/news/silicon-valley-power-says-data-center-load-to-double-by-2035-will-need-geothermal-power-and-batteries</a></p><p>[20]. Plumer, B. (2024). Three Mile Island Plans to Reopen as Demand for Nuclear Power Grows. https:/www.nytimes.com/2024/09/20/climate/three-mile-island-reopening.html</p><p>[21]. Sage Geosystems. (2025). Pioneering Pressure Geothermal. <a href=\"https://www.sagegeosystems.com/\">https://www.sagegeosystems.com/</a></p><p>[22]. Plumer, B., and Popovich, N. (2024). Giant Batteries Are Transforming the Way the U.S. Uses Electricity. <a href=\"https://www.nytimes.com/interactive/2024/05/07/climate/battery-electricity-solar-california-texas.html\">https://www.nytimes.com/interactive/2024/05/07/climate/battery-electricity-solar-california-texas.html</a></p><p>[23]. CALISO. (2025). <a href=\"https://www.caiso.com/\">https://www.caiso.com</a></p><p>[24]. Avalos, G. (2024). PG&amp;E bills could hop higher in 2026 due to plans tied to nuclear plant. <a href=\"https://www.mercurynews.com/2024/10/16/pge-bill-electric-utility-economy-consumer-diablo-canyon-nuclear-power/\">https://www.mercurynews.com/2024/10/16/pge-bill-electric-utility-economy-consumer-diablo-canyon-nuclear-power/</a></p><p>[25]. Weise, K. (2025). A.I., the Electricians and the Boom Towns of Central Washington. <a href=\"https://www.nytimes.com/2024/12/25/technology/ai-data-centers-electricians.html\">https://www.nytimes.com/2024/12/25/technology/ai-data-centers-electricians.html</a></p><p>[26]. Moss, S. (2025). Bell AI Fabric: Bell Canada plans AI \u2018data center supercluster\u2019 with 500MW in British Columbia. <a href=\"https://www.datacenterdynamics.com/en/news/bell-ai-fabric-bell-canada-plans-ai-data-center-supercluster-with-500mw-in-british-columbia\">https://www.datacenterdynamics.com/en/news/bell-ai-fabric-bell-canada-plans-ai-data-center-supercluster-with-500mw-in-british-columbia</a></p><p>[27]. International Renewable Energy Agency. (2025). <em>Renewable Energy Highlights 2025</em>. <a href=\"https://www.irena.org/-/media/Files/IRENA/Agency/Publication/2025/Jul/IRENA_DAT_Renewable_energy_highlights_2025.pdf\">https://www.irena.org/-/media/Files/IRENA/Agency/Publication/2025/Jul/IRENA_DAT_Renewable_energy_highlights_2025.pdf</a></p><p>[28]. Patel, D. et al. (2024). AI Datacenter Energy Dilemma\u200a\u2014\u200aRace for AI Datacenter Space. <a href=\"https://semianalysis.com/2024/03/13/ai-datacenter-energy-dilemma-race\">https://semianalysis.com/2024/03/13/ai-datacenter-energy-dilemma-race</a></p><p>[29]. Barnuevo, M. (2025). Data Center Energy Needs Could Upend Power Grids and Threaten the Climate. <a href=\"https://www.eesi.org/articles/view/data-center-energy-needs-are-upending-power-grids-and-threatening-the-climate\">https://www.eesi.org/articles/view/data-center-energy-needs-are-upending-power-grids-and-threatening-the-climate</a></p><p>[30]. IEA. (2025). <em>Energy and AI</em>. <a href=\"https://www.iea.org/reports/energy-and-ai/energy-supply-for-ai\">https://www.iea.org/reports/energy-and-ai/energy-supply-for-ai</a></p><p>[31]. Dalton, D. (2025). US And China To Lead Growth In Nuclear Power For Data Centre Supply. <a href=\"https://www.nucnet.org/news/us-and-china-to-lead-growth-in-nuclear-power-for-data-centre-supply-4-4-2025\">https://www.nucnet.org/news/us-and-china-to-lead-growth-in-nuclear-power-for-data-centre-supply-4-4-2025</a></p><p>[32]. Rogers, P. (2025). California reaches new record clean energy milestone. <a href=\"https://www.mercurynews.com/2025/07/14/california-reaches-new-record-clean-energy-milestone/\">https://www.mercurynews.com/2025/07/14/california-reaches-new-record-clean-energy-milestone/</a></p><p>[33]. LeVine, S. (2024). The Electric: China\u2019s Iron-Based Batteries Are Even Cheaper Than We Thought. <a href=\"https://www.theinformation.com/articles/the-electric-chinas-iron-based-batteries-are-even-cheaper-than-we-thought\">https://www.theinformation.com/articles/the-electric-chinas-iron-based-batteries-are-even-cheaper-than-we-thought</a></p><p>[34]. LeVine, S. (2024). The Electric: Low-Cost Batteries Have Made Some EVs as Cheap as Gas Cars. <a href=\"https://www.theinformation.com/articles/the-electric-cheaper-batteries-have-made-some-evs-as-cheap-as-gas-cars?rc=oqpr4p\">https://www.theinformation.com/articles/the-electric-cheaper-batteries-have-made-some-evs-as-cheap-as-gas-cars?rc=oqpr4p</a></p><p>[35]. Energy Markets &amp; Policy. (2025). Queued Up: Characteristics of Power Plants Seeking Transmission Interconnection. <a href=\"https://emp.lbl.gov/queues\">https://emp.lbl.gov/queues</a></p><p>[36]. O\u2019Brien, M. (2024). Ireland embraced data centers that the AI boom needs. Now they\u2019re consuming too much of its energy.<strong> </strong><a href=\"https://apnews.com/article/ai-data-centers-ireland-6c0d63cbda3df740cd9bf2829ad62058\">https://apnews.com/article/ai-data-centers-ireland-6c0d63cbda3df740cd9bf2829ad62058</a></p><p>[37]. Popovich, N., and Plumer, B. (2023). Why the U.S. Electric Grid Isn\u2019t Ready for the Energy Transition. <a href=\"https://www.nytimes.com/interactive/2023/06/12/climate/us-electric-grid-energy-transition.html\">https://www.nytimes.com/interactive/2023/06/12/climate/us-electric-grid-energy-transition.html</a></p><p>[38]. Felt. (2025). US Electric Power Transmission Lines in California. <a href=\"https://felt.com/map/US-Electric-Power-Transmission-Lines-in-California-vj8rtSJJRxmHG8t29AUPLJC?loc=37.272,-119.273,6z\">https://felt.com/map/US-Electric-Power-Transmission-Lines-in-California-vj8rtSJJRxmHG8t29AUPLJC?loc=37.272,-119.273,6z</a></p><p>[39]. Kuntz, J., and Kuntz, L. (2024). Opinion: AI is here. Get ready for a spike in your electric bill.<a href=\"https://www.cnn.com/2024/07/05/opinions/artificial-intelligence-electricity-grid-demand-kuntz/index.html\">https://www.cnn.com/2024/07/05/opinions/artificial-intelligence-electricity-grid-demand-kuntz/index.html</a></p><p>[40]. Lozano, J. (2023). Texas heat brings the state\u2019s power grid closest it has been to outages since 2021 winter storm.<a href=\"https://apnews.com/article/texas-power-grid-heat-emergency-alert-de76bc9fe6fd16e97ab6fc8d0c165065\">https://apnews.com/article/texas-power-grid-heat-emergency-alert-de76bc9fe6fd16e97ab6fc8d0c165065</a></p><p>[41]. US Department of Energy. (2025). August 2003 Blackout. <a href=\"https://www.energy.gov/oe/august-2003-blackout\">https://www.energy.gov/oe/august-2003-blackout</a></p><p>[42]. Whitehouse, S. (2023). Whitehouse, Quigley reintroduce bill to modernize the nation\u2019s electric grid. <a href=\"https://www.whitehouse.senate.gov/news/release/whitehouse-quigley-reintroduce-bill-to-modernize-the-nations-electric-grid-/\">https://www.whitehouse.senate.gov/news/release/whitehouse-quigley-reintroduce-bill-to-modernize-the-nations-electric-grid-/</a></p><p>[43]. CB Insights. (2025). <em>Big Tech in Energy</em>. <a href=\"https://www.cbinsights.com/reports/CB-Insights_Big-Tech-Energy.pdf\">https://www.cbinsights.com/reports/CB-Insights_Big-Tech-Energy.pdf</a></p><p>[44]. CB Insights. (2025). <em>Big Tech in Energy</em>. <a href=\"https://www.cbinsights.com/reports/CB-Insights_Big-Tech-Energy.pdf\">https://www.cbinsights.com/reports/CB-Insights_Big-Tech-Energy.pdf</a></p><p>[45]. McKinsey Global Institute. (2025). <em>The Hard Stuff: Navigating the physical realities of the energy transition</em>. <a href=\"https://www.mckinsey.com/~/media/mckinsey/mckinsey%20global%20institute/our%20research/the%20hard%20stuff%20navigating%20the%20physical%20realities%20of%20the%20energy%20transition/the-hard-stuff-navigating-the-physical-realities-of-the-energy-transition.pdf\">https://www.mckinsey.com/~/media/mckinsey/mckinsey-global-institute/our-research/the-hard-stuff-navigating-the-physical-realities-of-the-energy-transition/the-hard-stuff-navigating-the-physical-realities-of-the-energy-transition.pdf</a></p><p>[46]. McKinsey &amp; Company. (2024). <em>Beyond the hype: Capturing the potential of AI and gen AI in tech, media, and telecom.</em> <a href=\"https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/beyond-the-hype-capturing-the-potential-of-ai-and-gen-ai-in-tmt\">https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/beyond-the-hype-capturing-the-potential-of-ai-and-gen-ai-in-tmt</a></p><p>[47]. Sharma, G. (2024). Why Bill Gates Feels Energy Hungry AI Systems Are No Cause For Anxiety. <a href=\"https://www.forbes.com/sites/gauravsharma/2024/06/30/why-bill-gates-feels-energy-hungry-ai-systems-are-no-cause-for-anxiety/\">https://www.forbes.com/sites/gauravsharma/2024/06/30/why-bill-gates-feels-energy-hungry-ai-systems-are-no-cause-for-anxiety/</a></p><p>[48]. Khokha, S. (2024). Can Analog Chips Pave the Way for Sustainable AI? <a href=\"https://www.eetimes.com/can-analog-chips-pave-the-way-for-sustainable-ai/\">https://www.eetimes.com/can-analog-chips-pave-the-way-for-sustainable-ai/</a></p><p>[49]. Vaseghi, S. (2024). Breaking the AI Energy Barrier: The Rise of Thermodynamic Computing. <a href=\"https://ai.gopubby.com/breaking-the-ai-energy-barrier-the-rise-of-thermodynamic-computing-f796f079fd5f\">https://ai.gopubby.com/breaking-the-ai-energy-barrier-the-rise-of-thermodynamic-computing-f796f079fd5f</a></p><p>[50]. Fazekas, L. (2024). What is Thermodynamic Computing and how does it help AI development?! <a href=\"https://thebojda.medium.com/what-is-thermodynamic-computing-and-how-does-it-help-ai-development-1dd3b75a9ee6\">https://thebojda.medium.com/what-is-thermodynamic-computing-and-how-does-it-help-ai-development-1dd3b75a9ee6</a></p><p>[51]. Wikipedia contributors. (2025). Optical computing. <a href=\"https://en.wikipedia.org/wiki/Optical_computing\">https://en.wikipedia.org/wiki/Optical_computing</a></p><p>[52]. Khokha, S. (2024). Can Analog Chips Pave the Way for Sustainable AI <a href=\"https://www.eetimes.com/can-analog-chips-pave-the-way-for-sustainable-ai/\">https://www.eetimes.com/can-analog-chips-pave-the-way-for-sustainable-ai/</a></p><p>[53]. Dahad, N. (2025). Ambature to Deliver Superconductor IP for AI\u2019s Energy Problem. <a href=\"https://www.eetimes.com/ambature-to-deliver-superconductor-ip-for-ais-energy-problem\">https://www.eetimes.com/ambature-to-deliver-superconductor-ip-for-ais-energy-problem</a></p><p>[54]. Tom, D., and Svore, K. (2025). How Microsoft and Quantinuum achieved reliable quantum computing. <a href=\"https://cloudblogs.microsoft.com/quantum/2024/04/03/how-microsoft-and-quantinuum-achieved-reliable-quantum-computing/\">https://cloudblogs.microsoft.com/quantum/2024/04/03/how-microsoft-and-quantinuum-achieved-reliable-quantum-computing/</a></p><p>[55]. OpenAI. (2025). <em>AI in America: OpenAI\u2019s Economic Blueprint.</em> <a href=\"https://cdn.openai.com/global-affairs/ai-in-america-oai-economic-blueprint-20250113.pdf\">https://cdn.openai.com/global-affairs/ai-in-america-oai-economic-blueprint-20250113.pdf</a></p><p>[56]. Gray, C. (2025). OpenAI\u2019s Economic Blueprint\u200a\u2014\u200aSync #502. <a href=\"https://www.humanityredefined.com/p/sync-502\">https://www.humanityredefined.com/p/sync-502</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=752396678a6e\" width=\"1\" /><hr /><p><a href=\"https://ai.gopubby.com/ai-energy-paradox-powering-the-future-without-burning-it-down-752396678a6e\">AI Energy Paradox: Powering the Future Without Burning It Down</a> was originally published in <a href=\"https://ai.gopubby.com\">AI Advances</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "score": 0.107433,
    "pub_date": "2025-07-30T11:52:06+00:00",
    "theme": "opportunity",
    "category": "empowerment"
  },
  {
    "title": "Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding",
    "url": "https://arxiv.org/abs/2507.22378",
    "summary": "arXiv:2507.22378v1 Announce Type: cross \nAbstract: A fundamental challenge in neuroscience is to decode mental states from brain activity. While functional magnetic resonance imaging (fMRI) offers a non-invasive approach to capture brain-wide neural dynamics with high spatial precision, decoding from fMRI data -- particularly from task-evoked activity -- remains challenging due to its high dimensionality, low signal-to-noise ratio, and limited within-subject data. Here, we leverage recent advances in computer vision and propose STDA-SwiFT, a transformer-based model that learns transferable representations from large-scale fMRI datasets via spatial-temporal divided attention and self-supervised contrastive learning. Using pretrained voxel-wise representations from 995 subjects in the Human Connectome Project (HCP), we show that our model substantially improves downstream decoding performance of task-evoked activity across multiple sensory and cognitive domains, even with minimal data preprocessing. We demonstrate performance gains from larger receptor fields afforded by our memory-efficient attention mechanism, as well as the impact of functional relevance in pretraining data when fine-tuning on small samples. Our work showcases transfer learning as a viable approach to harness large-scale datasets to overcome challenges in decoding brain activity from fMRI data.",
    "score": 0.06182,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "science",
    "category": "neurobiology"
  },
  {
    "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis",
    "url": "https://arxiv.org/abs/2504.11257",
    "summary": "arXiv:2504.11257v4 Announce Type: replace \nAbstract: Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://microsoft.github.io/FIVE-UI-Evol/ .",
    "score": 0.027429,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "ux",
    "category": "human-computer-interface"
  },
  {
    "title": "RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots",
    "url": "https://arxiv.org/abs/2507.22664",
    "summary": "arXiv:2507.22664v1 Announce Type: cross \nAbstract: The presence of autonomous systems is growing at a fast pace and it is impacting many aspects of our lives. Designed to learn and act independently, these systems operate and perform decision-making without human intervention. However, they lack the ability to incorporate users' ethical preferences, which are unique for each individual in society and are required to personalize the decision-making processes. This reduces user trust and prevents autonomous systems from behaving according to the moral beliefs of their end-users. When multiple systems interact with differing ethical preferences, they must negotiate to reach an agreement that satisfies the ethical beliefs of all the parties involved and adjust their behavior consequently. To address this challenge, this paper proposes RobEthiChor, an approach that enables autonomous systems to incorporate user ethical preferences and contextual factors into their decision-making through ethics-based negotiation. RobEthiChor features a domain-agnostic reference architecture for designing autonomous systems capable of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an implementation of RobEthiChor within the Robot Operating System (ROS), which can be deployed on robots to provide them with ethics-based negotiation capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real robots and ran scenarios where a pair of robots negotiate upon resource contention. Experimental results demonstrate the feasibility and effectiveness of the system in realizing ethics-based negotiation. RobEthiChor allowed robots to reach an agreement in more than 73\\% of the scenarios with an acceptable negotiation time (0.67s on average). Experiments also demonstrate that the negotiation approach implemented in RobEthiChor is scalable.",
    "score": 0.011934,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "agency",
    "category": "ethics"
  },
  {
    "title": "Humanoid Robot Priced Under $6K Unveiled",
    "url": "https://aibusiness.com/automation/humanoid-robot-priced-under-6k-unveiled",
    "summary": "Heralded as a leap toward accessible humanoid robotics, the R1 undercuts competitors including Tesla, Figure AI and Apptronik",
    "score": 0.0,
    "pub_date": "2025-07-30T20:45:36+00:00",
    "theme": "agency",
    "category": "robots"
  },
  {
    "title": "Battleground: Selenium vs. Cypress vs. Playwright for Hunting Broken Links",
    "url": "https://ai.gopubby.com/battleground-selenium-vs-cypress-vs-playwright-for-hunting-broken-links-04d4907b7ef0?source=rss----3fe99b2acc4---4",
    "summary": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://ai.gopubby.com/battleground-selenium-vs-cypress-vs-playwright-for-hunting-broken-links-04d4907b7ef0?source=rss----3fe99b2acc4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*LxJDLRnmUyux1zaAE4PMLA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">After 15 years leading QA automation initiatives, I&#x2019;ve learned that the simplest tests often reveal the most about a framework&#x2019;s core&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://ai.gopubby.com/battleground-selenium-vs-cypress-vs-playwright-for-hunting-broken-links-04d4907b7ef0?source=rss----3fe99b2acc4---4\">Continue reading on AI Advances \u00bb</a></p></div>",
    "score": 0.0,
    "pub_date": "2025-07-30T19:27:01+00:00",
    "theme": "opportunity",
    "category": "monetization"
  },
  {
    "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?",
    "url": "https://arxiv.org/abs/2507.22099",
    "summary": "arXiv:2507.22099v1 Announce Type: new \nAbstract: Physics Engines (PEs) are fundamental software frameworks that simulate physical interactions in applications ranging from entertainment to safety-critical systems. Despite their importance, PEs suffer from physics failures, deviations from expected physical behaviors that can compromise software reliability, degrade user experience, and potentially cause critical failures in autonomous vehicles or medical robotics. Current testing approaches for PE-based software are inadequate, typically requiring white-box access and focusing on crash detection rather than semantically complex physics failures. This paper presents the first large-scale empirical study characterizing physics failures in PE-based software. We investigate three research questions addressing the manifestations of physics failures, the effectiveness of detection techniques, and developer perceptions of current detection practices. Our contributions include: (1) a taxonomy of physics failure manifestations; (2) a comprehensive evaluation of detection methods including deep learning, prompt-based techniques, and large multimodal models; and (3) actionable insights from developer experiences for improving detection approaches. To support future research, we release PhysiXFails, code, and other materials at https://sites.google.com/view/physics-failure-detection.",
    "score": 0.0,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "science",
    "category": "quantum"
  },
  {
    "title": "The role of media memorability in facilitating startups' access to venture capital funding",
    "url": "https://arxiv.org/abs/2507.22201",
    "summary": "arXiv:2507.22201v1 Announce Type: new \nAbstract: Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.",
    "score": 0.0,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "society",
    "category": "ai-integration"
  },
  {
    "title": "Analysis of User Experience Evaluation Methods for Deaf users: A Case Study on a mobile App",
    "url": "https://arxiv.org/abs/2507.22455",
    "summary": "arXiv:2507.22455v1 Announce Type: new \nAbstract: User Experience (UX) evaluation methods that are commonly used with hearing users may not be functional or effective for Deaf users. This is because these methods are primarily designed for users with hearing abilities, which can create limitations in the interaction, perception, and understanding of the methods for Deaf individuals. Furthermore, traditional UX evaluation approaches often fail to address the unique accessibility needs of Deaf users, resulting in an incomplete or biased assessment of their user experience. This research focused on analyzing a set of UX evaluation methods recommended for use with Deaf users, with the aim of validating the accessibility of each method through findings and limitations. The results indicate that, although these evaluation methods presented here are commonly recommended in the literature for use with Deaf users, they present various limitations that must be addressed in order to better adapt to the communication skills specific to the Deaf community. This research concludes that evaluation methods must be adapted to ensure accessible software evaluation for Deaf individuals, enabling the collection of data that accurately reflects their experiences and needs.",
    "score": 0.0,
    "pub_date": "2025-07-31T00:00:00-04:00",
    "theme": "ux",
    "category": "wearables"
  }
]