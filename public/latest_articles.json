[
  {
    "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
    "url": "https://arxiv.org/abs/2506.07106",
    "summary": "arXiv:2506.07106v2 Announce Type: replace \nAbstract: Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.",
    "score": 0.380505,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes",
    "url": "https://arxiv.org/abs/2507.22940",
    "summary": "arXiv:2507.22940v1 Announce Type: new \nAbstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy for Confidence Enhancement), a novel framework addressing a critical vulnerability in Large Language Models (LLMs): the prevalence of factual inaccuracies within intermediate reasoning steps despite correct final answers. This phenomenon poses substantial risks in high-stakes domains including healthcare, legal analysis, and scientific research, where erroneous yet confidently presented reasoning can mislead users into dangerous decisions. Our framework integrates three core components: (1) a specialized fact-checking classifier trained on counterfactually augmented data to detect subtle factual inconsistencies within reasoning chains; (2) a Group Relative Policy Optimization (GRPO) reinforcement learning approach that balances factuality, coherence, and structural correctness through multi-dimensional rewards; and (3) a mechanistic interpretability module examining how factuality improvements manifest in model activations during reasoning processes. Extensive evaluation across ten state-of-the-art models reveals concerning patterns: even leading models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual robustness (up to 49.90% improvement) while maintaining or improving performance on challenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore, our activation-level analysis provides actionable insights into how factual enhancements reshape reasoning trajectories within model architectures, establishing foundations for future training methodologies that explicitly target factual robustness through activation-guided optimization.",
    "score": 0.365514,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics",
    "url": "https://arxiv.org/abs/2506.12365",
    "summary": "arXiv:2506.12365v2 Announce Type: replace \nAbstract: This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.",
    "score": 0.345185,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication",
    "url": "https://arxiv.org/abs/2507.23247",
    "summary": "arXiv:2507.23247v1 Announce Type: new \nAbstract: There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs.",
    "score": 0.317732,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants",
    "url": "https://arxiv.org/abs/2507.22900",
    "summary": "arXiv:2507.22900v1 Announce Type: new \nAbstract: This exploratory study examines how AI code assistants shape novice programmers' experiences during a two-part exam in an introductory programming course. In the first part, students completed a programming task with access to AI support; in the second, they extended their solutions without AI. We collected Likert-scale and open-ended responses from 20 students to evaluate their perceptions and challenges. Findings suggest that AI tools were perceived as helpful for understanding code and increasing confidence, particularly during initial development. However, students reported difficulties transferring knowledge to unaided tasks, revealing possible overreliance and gaps in conceptual understanding. These insights highlight the need for pedagogical strategies that integrate AI meaningfully while reinforcing foundational programming skills.",
    "score": 0.308606,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "society",
    "category": "education"
  },
  {
    "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery",
    "url": "https://arxiv.org/abs/2507.23488",
    "summary": "arXiv:2507.23488v1 Announce Type: new \nAbstract: Causal inference remains a fundamental challenge for large language models. Recent advances in internal reasoning with large language models have sparked interest in whether state-of-the-art reasoning models can robustly perform causal discovery-a task where conventional models often suffer from severe overfitting and near-random performance under data perturbations. We study causal discovery on the Corr2Cause benchmark using the emergent OpenAI's o-series and DeepSeek-R model families and find that these reasoning-first architectures achieve significantly greater native gains than prior approaches. To capitalize on these strengths, we introduce a modular in-context pipeline inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding nearly three-fold improvements over conventional baselines. We further probe the pipeline's impact by analyzing reasoning chain length, complexity, and conducting qualitative and quantitative comparisons between conventional and reasoning models. Our findings suggest that while advanced reasoning models represent a substantial leap forward, carefully structured in-context frameworks are essential to maximize their capabilities and offer a generalizable blueprint for causal discovery across diverse domains.",
    "score": 0.303061,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration",
    "url": "https://arxiv.org/abs/2507.23407",
    "summary": "arXiv:2507.23407v1 Announce Type: new \nAbstract: Critical thinking is essential for building robust AI systems, preventing them from blindly accepting flawed data or biased reasoning. However, prior work has primarily focused on passive critical thinking, where models simply reject problematic queries without taking constructive steps to address user requests. In this work, we introduce proactive critical thinking, a paradigm where models actively seek missing or clarifying information from users to resolve their queries better. To evaluate this capability, we present GSM-MC and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math problems with a key variable deliberately removed, requiring models to identify and request the missing information. GSM-MCE further increases the difficulty by introducing irrelevant details to test robustness against distractions. Experiments on Qwen3 and Llama series models show that, while these models excel in traditional reasoning tasks due to extensive post-training and inference-time scaling, they struggle with proactive critical thinking, especially smaller ones. However, we demonstrate that reinforcement learning (RL) can significantly improve this ability. Using our enhanced RL algorithm, we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to 73.98% on GSM-MC. We hope this work advances models that collaborate more effectively with users in problem-solving through proactive critical thinking.",
    "score": 0.29269,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "cognition",
    "category": "reasoning"
  },
  {
    "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment",
    "url": "https://arxiv.org/abs/2401.13481",
    "summary": "arXiv:2401.13481v3 Announce Type: replace-cross \nAbstract: Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.",
    "score": 0.253958,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "agency",
    "category": "creativity"
  },
  {
    "title": "Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls",
    "url": "https://arxiv.org/abs/2507.22889",
    "summary": "arXiv:2507.22889v1 Announce Type: new \nAbstract: Conversations transform individual knowledge into collective insight, allowing groups of humans and increasingly groups of artificial intelligence (AI) agents to collaboratively solve complex problems. Whether interactions between AI agents can replicate the synergy observed in human discussions remains an open question. To investigate this, we systematically compared four conversational configurations: pairs of large language models (LLM-LLM), trios of LLMs, trios of humans, and mixed human-LLM pairs. After agents answered questions individually, they engaged in open-ended discussions and then reconsidered their initial answers. Interactions involving humans consistently led to accuracy improvements after the conversations, benefiting both stronger and weaker participants. By contrast, purely LLM-based pairs and trios exhibited declines in accuracy, demonstrating limited conversational synergy. Analysis of participants' confidence and answer-switching behavior revealed that knowledge diversity is a critical factor enabling collaborative improvement. Crucially, the lack of gains in LLM-LLM interactions did not stem from a fundamental limitation of the models' ability to collaborate, but from highly similar knowledge states that left little room for productive exchange. Our findings argue for a paradigm shift in AI development: rather than optimizing individual models solely for standalone performance, explicitly cultivating diversity across agents, even at the cost of slightly lower individual accuracy, may yield AI collaborators that are more effective in group settings with humans or other AI systems.",
    "score": 0.234477,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "ux",
    "category": "collaboration"
  },
  {
    "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey",
    "url": "https://arxiv.org/abs/2507.22934",
    "summary": "arXiv:2507.22934v1 Announce Type: new \nAbstract: Intent recognition aims to identify users' underlying intentions, traditionally focusing on text in natural language processing. With growing demands for natural human-computer interaction, the field has evolved through deep learning and multimodal approaches, incorporating data from audio, vision, and physiological signals. Recently, the introduction of Transformer-based models has led to notable breakthroughs in this domain. This article surveys deep learning methods for intent recognition, covering the shift from unimodal to multimodal techniques, relevant datasets, methodologies, applications, and current challenges. It provides researchers with insights into the latest developments in multimodal intent recognition (MIR) and directions for future research.",
    "score": 0.223667,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "ux",
    "category": "modalities"
  },
  {
    "title": "AI Must not be Fully Autonomous",
    "url": "https://arxiv.org/abs/2507.23330",
    "summary": "arXiv:2507.23330v1 Announce Type: new \nAbstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many risks. In this work, we identify the 3 levels of autonomous AI. We are of the position that AI must not be fully autonomous because of the many risks, especially as artificial superintelligence (ASI) is speculated to be just decades away. Fully autonomous AI, which can develop its own objectives, is at level 3 and without responsible human oversight. However, responsible human oversight is crucial for mitigating the risks. To ague for our position, we discuss theories of autonomy, AI and agents. Then, we offer 12 distinct arguments and 6 counterarguments with rebuttals to the counterarguments. We also present 15 pieces of recent evidence of AI misaligned values and other risks in the appendix.",
    "score": 0.221836,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "agency",
    "category": "ethics"
  },
  {
    "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure",
    "url": "https://arxiv.org/abs/2507.22893",
    "summary": "arXiv:2507.22893v1 Announce Type: new \nAbstract: Contemporary human-AI interaction research overlooks how AI systems fundamentally reshape human cognition pre-consciously, a critical blind spot for understanding distributed cognition. This paper introduces \"Cognitive Infrastructure Studies\" (CIS) as a new interdisciplinary domain to reconceptualize AI as \"cognitive infrastructures\": foundational, often invisible systems conditioning what is knowable and actionable in digital societies. These semantic infrastructures transport meaning, operate through anticipatory personalization, and exhibit adaptive invisibility, making their influence difficult to detect. Critically, they automate \"relevance judgment,\" shifting the \"locus of epistemic agency\" to non-human systems. Through narrative scenarios spanning individual (cognitive dependency), collective (democratic deliberation), and societal (governance) scales, we describe how cognitive infrastructures reshape human cognition, public reasoning, and social epistemologies. CIS aims to address how AI preprocessing reshapes distributed cognition across individual, collective, and cultural scales, requiring unprecedented integration of diverse disciplinary methods. The framework also addresses critical gaps across disciplines: cognitive science lacks population-scale preprocessing analysis capabilities, digital sociology cannot access individual cognitive mechanisms, and computational approaches miss cultural transmission dynamics. To achieve this goal CIS also provides methodological innovations for studying invisible algorithmic influence: \"infrastructure breakdown methodologies\", experimental approaches that reveal cognitive dependencies by systematically withdrawing AI preprocessing after periods of habituation.",
    "score": 0.200003,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "society",
    "category": "future"
  },
  {
    "title": "Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary",
    "url": "https://arxiv.org/abs/2507.23454",
    "summary": "arXiv:2507.23454v1 Announce Type: new \nAbstract: This article explores a critical gap in Mixed Reality (MR) technology: while advances have been made, MR still struggles to authentically replicate human embodiment and socio-motor interaction. For MR to enable truly meaningful social experiences, it needs to incorporate multi-modal data streams and multi-agent interaction capabilities. To address this challenge, we present a comprehensive glossary covering key topics such as Virtual Characters and Autonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges of Social MR within Neuroscience, Embodiment, and Technology. Our aim is to drive the transformative evolution of MR technologies that prioritize human-centric innovation, fostering richer digital connections. We advocate for MR systems that enhance social interaction and collaboration between humans and virtual autonomous agents, ensuring inclusivity, ethical design and psychological safety in the process.",
    "score": 0.170305,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "agency",
    "category": "companion"
  },
  {
    "title": "Answer Engine Optimization (AEO): I Love Markdown, So Does AI",
    "url": "https://ai.gopubby.com/answer-engine-optimization-aeo-i-love-markdown-so-does-ai-85eee65cbeb7?source=rss----3fe99b2acc4---4",
    "summary": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://ai.gopubby.com/answer-engine-optimization-aeo-i-love-markdown-so-does-ai-85eee65cbeb7?source=rss----3fe99b2acc4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*eEaXX0hqeYEY3Z7tr2Mfxg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">The open secret of Human-AI Collaboration of reading and writing in Markdown</p><p class=\"medium-feed-link\"><a href=\"https://ai.gopubby.com/answer-engine-optimization-aeo-i-love-markdown-so-does-ai-85eee65cbeb7?source=rss----3fe99b2acc4---4\">Continue reading on AI Advances \u00bb</a></p></div>",
    "score": 0.167084,
    "pub_date": "2025-07-31T13:11:11+00:00",
    "theme": "opportunity",
    "category": "empowerment"
  },
  {
    "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web",
    "url": "https://arxiv.org/abs/2507.23585",
    "summary": "arXiv:2507.23585v1 Announce Type: new \nAbstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI tools, often prioritize engagement and efficiency at the expense of user agency. As systems take on more decision-making, users have less control over what they see and how meaning or relationships between content are constructed. This paper introduces \"Hypertextual Friction,\" a conceptual design stance that repositions classical hypertext principles--friction, traceability, and structure--as actionable values for reclaiming agency in algorithmically mediated environments. Through a comparative analysis of real-world interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image tools--we examine how different systems structure user experience, navigation, and authorship. We show that hypertext systems emphasize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to obscure process and flatten participation. We contribute: (1) a comparative analysis of how interface structures shape agency in user-driven versus agent-driven systems, and (2) a conceptual stance that offers hypertextual values as design commitments for reclaiming agency in an increasingly algorithmic web.",
    "score": 0.113952,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "ux",
    "category": "research-assistant"
  },
  {
    "title": "Zuckerberg Unveils 'Personal Superintelligence' Vision as Meta Spends Big",
    "url": "https://aibusiness.com/meta/zuckerberg-unveils-personal-superintelligence-vision-as-meta-spends-big",
    "summary": "Meta CEO's new AI manifesto outlines individual empowerment strategy while company commits up to $72 billion to AI infrastructure",
    "score": 0.088964,
    "pub_date": "2025-07-31T16:53:04+00:00",
    "theme": "opportunity",
    "category": "monetization"
  },
  {
    "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation",
    "url": "https://arxiv.org/abs/2507.22892",
    "summary": "arXiv:2507.22892v1 Announce Type: new \nAbstract: Conventional augmentative and alternative communication (AAC) systems and language-learning platforms often fail to adapt in real time to the user's cognitive and linguistic needs, especially in neurological conditions such as post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in noninvasive electroencephalography (EEG)--based brain-computer interfaces (BCIs) and transformer--based large language models (LLMs) offer complementary strengths: BCIs capture users' neural intent with low fatigue, while LLMs generate contextually tailored language content. We propose and evaluate a novel hybrid framework that leverages real-time EEG signals to drive an LLM-powered language rehabilitation assistant. This system aims to: (1) enable users with severe speech or motor impairments to navigate language-learning modules via mental commands; (2) dynamically personalize vocabulary, sentence-construction exercises, and corrective feedback; and (3) monitor neural markers of cognitive effort to adjust task difficulty on the fly.",
    "score": 0.081141,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "ux",
    "category": "human-computer-interface"
  },
  {
    "title": "Machine learning and machine learned prediction in chest X-ray images",
    "url": "https://arxiv.org/abs/2507.23455",
    "summary": "arXiv:2507.23455v1 Announce Type: new \nAbstract: Machine learning and artificial intelligence are fast-growing fields of research in which data is used to train algorithms, learn patterns, and make predictions. This approach helps to solve seemingly intricate problems with significant accuracy without explicit programming by recognizing complex relationships in data. Taking an example of 5824 chest X-ray images, we implement two machine learning algorithms, namely, a baseline convolutional neural network (CNN) and a DenseNet-121, and present our analysis in making machine-learned predictions in predicting patients with ailments. Both baseline CNN and DenseNet-121 perform very well in the binary classification problem presented in this work. Gradient-weighted class activation mapping shows that DenseNet-121 correctly focuses on essential parts of the input chest X-ray images in its decision-making more than the baseline CNN.",
    "score": 0.074164,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "society",
    "category": "healthcare"
  },
  {
    "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments",
    "url": "https://arxiv.org/abs/2507.23190",
    "summary": "arXiv:2507.23190v1 Announce Type: new \nAbstract: Assessing the accessibility of unfamiliar built environments is critical for people with disabilities. However, manual assessments, performed by users or their personal health professionals, are laborious and unscalable, while automatic machine learning methods often neglect an individual user's unique needs. Recent advances in Large Language Models (LLMs) enable novel approaches to this problem, balancing personalization with scalability to enable more adaptive and context-aware assessments of accessibility. We present Accessibility Scout, an LLM-based accessibility scanning system that identifies accessibility concerns from photos of built environments. With use, Accessibility Scout becomes an increasingly capable \"accessibility scout\", tailoring accessibility scans to an individual's mobility level, preferences, and specific environmental interests through collaborative Human-AI assessments. We present findings from three studies: a formative study with six participants to inform the design of Accessibility Scout, a technical evaluation of 500 images of built environments, and a user study with 10 participants of varying mobility. Results from our technical evaluation and user study show that Accessibility Scout can generate personalized accessibility scans that extend beyond traditional ADA considerations. Finally, we conclude with a discussion on the implications of our work and future steps for building more scalable and personalized accessibility assessments of the physical world.",
    "score": 0.06977,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "ux",
    "category": "wearables"
  },
  {
    "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation",
    "url": "https://arxiv.org/abs/2507.22944",
    "summary": "arXiv:2507.22944v1 Announce Type: new \nAbstract: This article redefines arbitrariness not as a normative flaw or a symptom of domination, but as a foundational functional mechanism structuring human systems and interactions. Diverging from critical traditions that conflate arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a property enabling systems - linguistic, legal, or social - to operate effectively while withholding their internal rationale. Building on Ferdinand de Saussure's concept of l'arbitraire du signe, the analysis extends this principle beyond language to demonstrate its cross-domain applicability, particularly in law and social dynamics. The paper introduces the \"Motivation -> Constatability -> Contestability\" chain, arguing that motivation functions as a crucial interface rendering an act's logic vulnerable to intersubjective contestation. When this chain is broken through mechanisms like \"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the wolf drowned in the fish\"), acts produce binding effects without exposing their rationale, thus precluding justiciability. This structural opacity, while appearing illogical, is a deliberate design protecting authority from accountability. Drawing on Shannon's entropy model, the paper formalizes arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern theory of arbitrariness as a neutral operator central to control as well as care, an overlooked dimension of interpersonal relations. While primarily developed through human social systems, this framework also illuminates a new pathway for analyzing explainability in advanced artificial intelligence systems.",
    "score": 0.053488,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "science",
    "category": "quantum"
  },
  {
    "title": "Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface",
    "url": "https://arxiv.org/abs/2507.22895",
    "summary": "arXiv:2507.22895v1 Announce Type: new \nAbstract: Brain-computer interfaces (BCIs) enable real-time interaction between the brain and external devices by decoding neural signals. However, existing motor-based BCI paradigms, like motor imagery BCI, face challenges with imprecise labeling in real-world use. This mismatch between EEG signals and true behavioral intentions leads to pseudo-labels, undermining decoding accuracy and system robustness. To overcome this bottleneck, this paper first proposes a novel motor intention extraction framework based on a non-invasive brain-muscle interface (BMuI)($\\text{BCI} = \\frac{\\text{Brain}}{\\text{Computer}} \\text{ Interface} = \\frac{\\text{Brain}}{\\not\\text{Muscle}}\\! \\text{ (BMuI)} \\times \\!\\frac{\\not\\text{Muscle}}{\\text{Computer}}\\! \\text{ Interface}$). This method simulates the neural pathway from the brain to the muscles in order to capture and enhance the weak motor intention signals originating in the brain. It then uses EMG as a high-fidelity relay medium to achieve more accurate intention recognition and transmission. To systematically validate the feasibility and effectiveness of this approach, we conducted both offline experiments (to repeatedly verify feasibility) and online experiments (to construct a real-time interactive system and evaluate its performance). The results show that BMuI is feasible, achieving a prediction accuracy of 0.8314; in the online experiment, all participants are able to successfully control the Unity virtual arm.",
    "score": 0.0,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "science",
    "category": "neurobiology"
  },
  {
    "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment",
    "url": "https://arxiv.org/abs/2507.22922",
    "summary": "arXiv:2507.22922v1 Announce Type: new \nAbstract: The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.",
    "score": 0.0,
    "pub_date": "2025-08-01T00:00:00-04:00",
    "theme": "society",
    "category": "ai-integration"
  },
  {
    "title": "What is personalized pricing, and how do I avoid it?",
    "url": "https://theconversation.com/what-is-personalized-pricing-and-how-do-i-avoid-it-262195",
    "summary": "As AI powers individualized pricing across industries, businesses are increasingly charging based on what they think you can pay.",
    "score": 0.0,
    "pub_date": "2025-07-31T12:34:57+00:00",
    "theme": "society",
    "category": "work-transformation"
  },
  {
    "title": "Someone Gave ChatGPT $100 and Let Trade Stocks for a Month",
    "url": "https://futurism.com/chatgpt-stocks-100-dollars",
    "summary": "<p>With $100 and a dream, one enterprising Redditor turned ChatGPT into a day trader, and the results so far have been pretty remarkable. In a post on r/Dataisbeautiful, the Redditor in question \u2014 real name Nathan Smith \u2014 described his project as a \"6-month experiment to see how a language model performs in picking small, [under-covered] stocks with only a $100 budget.\" According to a chart shared on Reddit, this literal gamble is already paying off. Using GPT-4o, one of OpenAI's most advanced models, the bot-trader's stock portfolio has increased in value by 25 percent over its first month \u2014 [\u2026]</p>",
    "score": 0.0,
    "pub_date": "2025-07-31T13:04:16+00:00",
    "theme": "ux",
    "category": "search"
  }
]